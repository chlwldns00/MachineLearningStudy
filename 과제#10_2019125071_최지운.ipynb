{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cde0c3",
   "metadata": {},
   "source": [
    "# 오늘은 LeNet 구조를 만들어봅시다\n",
    "\n",
    "\n",
    "LeNet 구조는 CNN이며, 초기에 만들어진 모델입니다. \n",
    "\n",
    "2가지 모델(Sigmoid, ReLU)를 만들어 두 모델의 성능을 비교해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aff3fd",
   "metadata": {},
   "source": [
    "## 1.우선 필요 라이브러리를 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bac6b",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9880ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.0+cpu  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5590af",
   "metadata": {},
   "source": [
    "## 3. MNIST 데이터 다운로드 \n",
    "\n",
    " 1. Training data와 Test data 분리하기\n",
    " \n",
    " 2. Training data를 Training data 와 Validation data로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00908077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:27<00:00, 361240.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 285397.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:02<00:00, 709671.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize([32, 32]), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5,), (1.0,))\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_data, [50000, 10000])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ffb80",
   "metadata": {},
   "source": [
    "## 4. torch.nn을 이용하여 모델-1 만들기\n",
    "\n",
    "   1) 아래의 그림 중 LeNet 구조를 구현 할 것\n",
    "   \n",
    "   2) Sigmoid 활성화 함수를 이용할 것\n",
    "   \n",
    "   \n",
    "![](Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defacffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sig4 = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.sig5 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sig1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sig2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sig3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sig4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sig5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26eed9",
   "metadata": {},
   "source": [
    "## 5. torch.nn을 이용하여 모델-2 만들기\n",
    "\n",
    "   LeNet 모델에서 ReLU 활성화 함수를 사용하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ac70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool2d(2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de556825",
   "metadata": {},
   "source": [
    "## 7. 학습 준비하기\n",
    "\n",
    "1) 1 epoch를 학습할 수 있는 함수 만들기\n",
    "\n",
    "2) Test와 Validation data의 정확도 계산할 수 있는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06030b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(train_loader, network, loss_func, optimizer, epoch):\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    log_interval = 300\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # 미분값의 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagration 계산하기.\n",
    "        outputs = network(image)\n",
    "        \n",
    "        \n",
    "        # Cross_entropy 함수를 적용하여 loss를 구하고 저장하기\n",
    "        loss = loss_func(outputs, label)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # training accuracy 정확도 구하기 위해 맞는 샘플 개수 세기\n",
    "        pred = outputs.data.argmax(dim=1)\n",
    "        train_correct += pred.eq(label).sum()\n",
    "\n",
    "        # Gradinet 구하기\n",
    "        loss.backward()\n",
    "\n",
    "        # weight값 update 하기\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습 상황 출력\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(label), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))\n",
    "            \n",
    "    return train_losses, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(test_loader, network, loss_func, val = False):\n",
    "    correct = 0\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Forward propagration 계산하기.\n",
    "            outputs = network(image)\n",
    "\n",
    "            # Cross_entropy 함수를 적용하여 loss를 구하기\n",
    "            loss = loss_func(outputs, label)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Batch 별로 정확도 구하기\n",
    "            pred = outputs.data.argmax(dim=1)\n",
    "            correct += pred.eq(label).sum()\n",
    "\n",
    "        # 전체 정확도 구하기\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        #중간결과 출력\n",
    "        if val is True:\n",
    "                print('Validation set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "              .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        else:\n",
    "            print('Test set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "                  .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_losses, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c53",
   "metadata": {},
   "source": [
    "## 8. 위 정의된 함수로 학습 함수 만들기\n",
    "\n",
    "Adam Optimizer를 사용하여 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df29783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, learning_rate = 0.001):\n",
    "    \n",
    "    epoches = 15\n",
    "    \n",
    "    cls_loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "    \n",
    "    train_losses_per_epoch = []\n",
    "    test_losses_per_epoch = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "                \n",
    "        # 모델를 학습 중이라고 선언하기\n",
    "        network.train()\n",
    "        \n",
    "        train_losses, train_correct = training_epoch(train_loader, network, cls_loss, optimizer, epoch)\n",
    "        \n",
    "        # epoch 별로 loss 평균값, 정확도 구하기\n",
    "        average_loss = np.mean(train_losses)\n",
    "        train_losses_per_epoch.append(average_loss)\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # epoch 별로 정확도 출력\n",
    "        print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)'\n",
    "              .format(train_correct, len(train_loader.dataset),100. * train_correct / len(train_loader.dataset)))\n",
    "\n",
    "        \n",
    "        ### 학습 중에 test 결과 보기\n",
    "        \n",
    "        # 모델 test 중인 것을 선언하기\n",
    "        network.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            test_losses, test_accuracy = test_epoch(val_loader, network, cls_loss, True)\n",
    "\n",
    "        test_losses_per_epoch.append(np.mean(test_losses))\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_losses, test_accuracy = test_epoch(test_loader, network, cls_loss, False)\n",
    "        \n",
    "    return train_losses_per_epoch, test_losses_per_epoch, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1394321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0.00%)]\tLoss: 2.300797\n",
      "Train Epoch: 0 [19200/60000 (31.98%)]\tLoss: 1.975947\n",
      "Train Epoch: 0 [38400/60000 (63.97%)]\tLoss: 1.690604\n",
      "Train Epoch: 0 [57600/60000 (95.95%)]\tLoss: 1.611830\n",
      "\n",
      "Training set: Accuracy: 36865/60000 (61.44%)\n",
      "Validation set: Accuracy: 8276/10000 (82.76%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0.00%)]\tLoss: 1.646723\n",
      "Train Epoch: 1 [19200/60000 (31.98%)]\tLoss: 1.579133\n",
      "Train Epoch: 1 [38400/60000 (63.97%)]\tLoss: 1.538856\n",
      "Train Epoch: 1 [57600/60000 (95.95%)]\tLoss: 1.551006\n",
      "\n",
      "Training set: Accuracy: 51339/60000 (85.57%)\n",
      "Validation set: Accuracy: 8484/10000 (84.84%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0.00%)]\tLoss: 1.551437\n",
      "Train Epoch: 2 [19200/60000 (31.98%)]\tLoss: 1.549359\n",
      "Train Epoch: 2 [38400/60000 (63.97%)]\tLoss: 1.512968\n",
      "Train Epoch: 2 [57600/60000 (95.95%)]\tLoss: 1.528991\n",
      "\n",
      "Training set: Accuracy: 51528/60000 (85.88%)\n",
      "Validation set: Accuracy: 8566/10000 (85.66%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0.00%)]\tLoss: 1.523599\n",
      "Train Epoch: 3 [19200/60000 (31.98%)]\tLoss: 1.537731\n",
      "Train Epoch: 3 [38400/60000 (63.97%)]\tLoss: 1.501438\n",
      "Train Epoch: 3 [57600/60000 (95.95%)]\tLoss: 1.523233\n",
      "\n",
      "Training set: Accuracy: 52365/60000 (87.28%)\n",
      "Validation set: Accuracy: 8818/10000 (88.18%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0.00%)]\tLoss: 1.508133\n",
      "Train Epoch: 4 [19200/60000 (31.98%)]\tLoss: 1.526551\n",
      "Train Epoch: 4 [38400/60000 (63.97%)]\tLoss: 1.496452\n",
      "Train Epoch: 4 [57600/60000 (95.95%)]\tLoss: 1.520868\n",
      "\n",
      "Training set: Accuracy: 54778/60000 (91.30%)\n",
      "Validation set: Accuracy: 9423/10000 (94.23%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0.00%)]\tLoss: 1.496417\n",
      "Train Epoch: 5 [19200/60000 (31.98%)]\tLoss: 1.507798\n",
      "Train Epoch: 5 [38400/60000 (63.97%)]\tLoss: 1.491526\n",
      "Train Epoch: 5 [57600/60000 (95.95%)]\tLoss: 1.512830\n",
      "\n",
      "Training set: Accuracy: 57526/60000 (95.88%)\n",
      "Validation set: Accuracy: 9591/10000 (95.91%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0.00%)]\tLoss: 1.482845\n",
      "Train Epoch: 6 [19200/60000 (31.98%)]\tLoss: 1.493700\n",
      "Train Epoch: 6 [38400/60000 (63.97%)]\tLoss: 1.488077\n",
      "Train Epoch: 6 [57600/60000 (95.95%)]\tLoss: 1.509569\n",
      "\n",
      "Training set: Accuracy: 57992/60000 (96.65%)\n",
      "Validation set: Accuracy: 9685/10000 (96.85%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0.00%)]\tLoss: 1.479398\n",
      "Train Epoch: 7 [19200/60000 (31.98%)]\tLoss: 1.489545\n",
      "Train Epoch: 7 [38400/60000 (63.97%)]\tLoss: 1.485648\n",
      "Train Epoch: 7 [57600/60000 (95.95%)]\tLoss: 1.506564\n",
      "\n",
      "Training set: Accuracy: 58245/60000 (97.07%)\n",
      "Validation set: Accuracy: 9725/10000 (97.25%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0.00%)]\tLoss: 1.475815\n",
      "Train Epoch: 8 [19200/60000 (31.98%)]\tLoss: 1.486897\n",
      "Train Epoch: 8 [38400/60000 (63.97%)]\tLoss: 1.483907\n",
      "Train Epoch: 8 [57600/60000 (95.95%)]\tLoss: 1.503083\n",
      "\n",
      "Training set: Accuracy: 58413/60000 (97.36%)\n",
      "Validation set: Accuracy: 9760/10000 (97.60%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0.00%)]\tLoss: 1.471203\n",
      "Train Epoch: 9 [19200/60000 (31.98%)]\tLoss: 1.485541\n",
      "Train Epoch: 9 [38400/60000 (63.97%)]\tLoss: 1.482388\n",
      "Train Epoch: 9 [57600/60000 (95.95%)]\tLoss: 1.498520\n",
      "\n",
      "Training set: Accuracy: 58577/60000 (97.63%)\n",
      "Validation set: Accuracy: 9794/10000 (97.94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0.00%)]\tLoss: 1.470338\n",
      "Train Epoch: 10 [19200/60000 (31.98%)]\tLoss: 1.484932\n",
      "Train Epoch: 10 [38400/60000 (63.97%)]\tLoss: 1.481266\n",
      "Train Epoch: 10 [57600/60000 (95.95%)]\tLoss: 1.497007\n",
      "\n",
      "Training set: Accuracy: 58701/60000 (97.83%)\n",
      "Validation set: Accuracy: 9802/10000 (98.02%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0.00%)]\tLoss: 1.469203\n",
      "Train Epoch: 11 [19200/60000 (31.98%)]\tLoss: 1.483284\n",
      "Train Epoch: 11 [38400/60000 (63.97%)]\tLoss: 1.480648\n",
      "Train Epoch: 11 [57600/60000 (95.95%)]\tLoss: 1.495334\n",
      "\n",
      "Training set: Accuracy: 58805/60000 (98.01%)\n",
      "Validation set: Accuracy: 9825/10000 (98.25%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0.00%)]\tLoss: 1.467435\n",
      "Train Epoch: 12 [19200/60000 (31.98%)]\tLoss: 1.481820\n",
      "Train Epoch: 12 [38400/60000 (63.97%)]\tLoss: 1.479367\n",
      "Train Epoch: 12 [57600/60000 (95.95%)]\tLoss: 1.491757\n",
      "\n",
      "Training set: Accuracy: 58901/60000 (98.17%)\n",
      "Validation set: Accuracy: 9842/10000 (98.42%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0.00%)]\tLoss: 1.465834\n",
      "Train Epoch: 13 [19200/60000 (31.98%)]\tLoss: 1.481305\n",
      "Train Epoch: 13 [38400/60000 (63.97%)]\tLoss: 1.477381\n",
      "Train Epoch: 13 [57600/60000 (95.95%)]\tLoss: 1.488564\n",
      "\n",
      "Training set: Accuracy: 58971/60000 (98.29%)\n",
      "Validation set: Accuracy: 9865/10000 (98.65%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0.00%)]\tLoss: 1.465108\n",
      "Train Epoch: 14 [19200/60000 (31.98%)]\tLoss: 1.480000\n",
      "Train Epoch: 14 [38400/60000 (63.97%)]\tLoss: 1.476373\n",
      "Train Epoch: 14 [57600/60000 (95.95%)]\tLoss: 1.487403\n",
      "\n",
      "Training set: Accuracy: 59061/60000 (98.43%)\n",
      "Validation set: Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Test set: Accuracy: 9835/10000 (98.35%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_1().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64815daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0.00%)]\tLoss: 2.302119\n",
      "Train Epoch: 0 [19200/60000 (31.98%)]\tLoss: 1.052400\n",
      "Train Epoch: 0 [38400/60000 (63.97%)]\tLoss: 0.872564\n",
      "Train Epoch: 0 [57600/60000 (95.95%)]\tLoss: 0.834742\n",
      "\n",
      "Training set: Accuracy: 38610/60000 (64.35%)\n",
      "Validation set: Accuracy: 6807/10000 (68.07%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0.00%)]\tLoss: 0.730340\n",
      "Train Epoch: 1 [19200/60000 (31.98%)]\tLoss: 0.942641\n",
      "Train Epoch: 1 [38400/60000 (63.97%)]\tLoss: 0.709464\n",
      "Train Epoch: 1 [57600/60000 (95.95%)]\tLoss: 0.778478\n",
      "\n",
      "Training set: Accuracy: 41262/60000 (68.77%)\n",
      "Validation set: Accuracy: 6926/10000 (69.26%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0.00%)]\tLoss: 0.671503\n",
      "Train Epoch: 2 [19200/60000 (31.98%)]\tLoss: 0.920980\n",
      "Train Epoch: 2 [38400/60000 (63.97%)]\tLoss: 0.650530\n",
      "Train Epoch: 2 [57600/60000 (95.95%)]\tLoss: 0.740954\n",
      "\n",
      "Training set: Accuracy: 41709/60000 (69.51%)\n",
      "Validation set: Accuracy: 7012/10000 (70.12%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0.00%)]\tLoss: 0.657821\n",
      "Train Epoch: 3 [19200/60000 (31.98%)]\tLoss: 0.908680\n",
      "Train Epoch: 3 [38400/60000 (63.97%)]\tLoss: 0.640355\n",
      "Train Epoch: 3 [57600/60000 (95.95%)]\tLoss: 0.706121\n",
      "\n",
      "Training set: Accuracy: 41943/60000 (69.90%)\n",
      "Validation set: Accuracy: 7038/10000 (70.38%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0.00%)]\tLoss: 0.651357\n",
      "Train Epoch: 4 [19200/60000 (31.98%)]\tLoss: 0.901778\n",
      "Train Epoch: 4 [38400/60000 (63.97%)]\tLoss: 0.637335\n",
      "Train Epoch: 4 [57600/60000 (95.95%)]\tLoss: 0.687414\n",
      "\n",
      "Training set: Accuracy: 42056/60000 (70.09%)\n",
      "Validation set: Accuracy: 7053/10000 (70.53%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0.00%)]\tLoss: 0.623961\n",
      "Train Epoch: 5 [19200/60000 (31.98%)]\tLoss: 0.886719\n",
      "Train Epoch: 5 [38400/60000 (63.97%)]\tLoss: 0.631568\n",
      "Train Epoch: 5 [57600/60000 (95.95%)]\tLoss: 0.695894\n",
      "\n",
      "Training set: Accuracy: 42150/60000 (70.25%)\n",
      "Validation set: Accuracy: 7061/10000 (70.61%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0.00%)]\tLoss: 0.619693\n",
      "Train Epoch: 6 [19200/60000 (31.98%)]\tLoss: 0.873735\n",
      "Train Epoch: 6 [38400/60000 (63.97%)]\tLoss: 0.633351\n",
      "Train Epoch: 6 [57600/60000 (95.95%)]\tLoss: 0.682416\n",
      "\n",
      "Training set: Accuracy: 42216/60000 (70.36%)\n",
      "Validation set: Accuracy: 7070/10000 (70.70%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0.00%)]\tLoss: 0.617753\n",
      "Train Epoch: 7 [19200/60000 (31.98%)]\tLoss: 0.877914\n",
      "Train Epoch: 7 [38400/60000 (63.97%)]\tLoss: 0.634583\n",
      "Train Epoch: 7 [57600/60000 (95.95%)]\tLoss: 0.661648\n",
      "\n",
      "Training set: Accuracy: 42285/60000 (70.47%)\n",
      "Validation set: Accuracy: 7070/10000 (70.70%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0.00%)]\tLoss: 0.626110\n",
      "Train Epoch: 8 [19200/60000 (31.98%)]\tLoss: 0.874951\n",
      "Train Epoch: 8 [38400/60000 (63.97%)]\tLoss: 0.624066\n",
      "Train Epoch: 8 [57600/60000 (95.95%)]\tLoss: 0.646936\n",
      "\n",
      "Training set: Accuracy: 42327/60000 (70.54%)\n",
      "Validation set: Accuracy: 7087/10000 (70.87%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0.00%)]\tLoss: 0.623671\n",
      "Train Epoch: 9 [19200/60000 (31.98%)]\tLoss: 0.875463\n",
      "Train Epoch: 9 [38400/60000 (63.97%)]\tLoss: 0.635905\n",
      "Train Epoch: 9 [57600/60000 (95.95%)]\tLoss: 0.622448\n",
      "\n",
      "Training set: Accuracy: 42362/60000 (70.60%)\n",
      "Validation set: Accuracy: 7081/10000 (70.81%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0.00%)]\tLoss: 0.623241\n",
      "Train Epoch: 10 [19200/60000 (31.98%)]\tLoss: 0.864808\n",
      "Train Epoch: 10 [38400/60000 (63.97%)]\tLoss: 0.618487\n",
      "Train Epoch: 10 [57600/60000 (95.95%)]\tLoss: 0.645790\n",
      "\n",
      "Training set: Accuracy: 42377/60000 (70.63%)\n",
      "Validation set: Accuracy: 7078/10000 (70.78%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0.00%)]\tLoss: 0.615834\n",
      "Train Epoch: 11 [19200/60000 (31.98%)]\tLoss: 0.868663\n",
      "Train Epoch: 11 [38400/60000 (63.97%)]\tLoss: 0.613017\n",
      "Train Epoch: 11 [57600/60000 (95.95%)]\tLoss: 0.617871\n",
      "\n",
      "Training set: Accuracy: 42411/60000 (70.68%)\n",
      "Validation set: Accuracy: 7102/10000 (71.02%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0.00%)]\tLoss: 0.613139\n",
      "Train Epoch: 12 [19200/60000 (31.98%)]\tLoss: 0.864158\n",
      "Train Epoch: 12 [38400/60000 (63.97%)]\tLoss: 0.636901\n",
      "Train Epoch: 12 [57600/60000 (95.95%)]\tLoss: 0.613198\n",
      "\n",
      "Training set: Accuracy: 42416/60000 (70.69%)\n",
      "Validation set: Accuracy: 7108/10000 (71.08%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0.00%)]\tLoss: 0.612788\n",
      "Train Epoch: 13 [19200/60000 (31.98%)]\tLoss: 0.864215\n",
      "Train Epoch: 13 [38400/60000 (63.97%)]\tLoss: 0.617615\n",
      "Train Epoch: 13 [57600/60000 (95.95%)]\tLoss: 0.616183\n",
      "\n",
      "Training set: Accuracy: 42426/60000 (70.71%)\n",
      "Validation set: Accuracy: 7094/10000 (70.94%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0.00%)]\tLoss: 0.612253\n",
      "Train Epoch: 14 [19200/60000 (31.98%)]\tLoss: 0.865228\n",
      "Train Epoch: 14 [38400/60000 (63.97%)]\tLoss: 0.613014\n",
      "Train Epoch: 14 [57600/60000 (95.95%)]\tLoss: 0.612424\n",
      "\n",
      "Training set: Accuracy: 42429/60000 (70.71%)\n",
      "Validation set: Accuracy: 7113/10000 (71.13%)\n",
      "\n",
      "Test set: Accuracy: 7051/10000 (70.51%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_2().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1471",
   "metadata": {},
   "source": [
    "## 9. 두모델의 성능을 비교하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8156f",
   "metadata": {},
   "source": [
    "정답)ReLu를 사용한 모델은 첫 epoch부터 Loss값이 Sigmoid를 사용한 모델에 비해 적게 나왔다. 하지만 학습을 진행할수록 계속해서 낮게 나오는 Loss값에 비해 Accuracy 향상이 상대적으로 더디게 일어났고, 반면에 Sigmoid 모델을 학습이 진행될수록 Accuracy가 급격하게 향상되었다.\n",
    "Sigmoid를 사용한 모델이 더 좋은 성능을 보여주고, Loss값과 Accuracy는 무조건적인 반비례 관계에 있지는 않다는 것을 알 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68b6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
